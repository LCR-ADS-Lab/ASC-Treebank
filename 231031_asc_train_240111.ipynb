{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCR-ADS-Lab/ASC-Treebank/blob/main/231031_asc_train_240111.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "rzy1AtdIF3af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "id": "6VEyAeu1SD8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "kSmeik-C4_yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_corpus(filename, output_filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        line = line.strip()  # Remove spaces from the beginning and the end of the line\n",
        "        if not line.startswith(\"#\"):\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) > 9:  # Check if there are at least 10 columns\n",
        "                parts[9] = parts[9].strip()  # Remove redundant spaces from the 10th column\n",
        "                line = \"\\t\".join(parts)\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    with open(output_filename, 'w') as file:\n",
        "        file.write(\"\\n\".join(cleaned_lines))\n",
        "\n",
        "filename = \"/content/silverSentences_database20231128.txt\"\n",
        "cleaned_filename = \"/content/cleaned_silverSentences_database20231128.txt\"\n",
        "\n",
        "clean_corpus(filename, cleaned_filename)"
      ],
      "metadata": {
        "id": "UvTNsTa7BhB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc, Token\n",
        "from spacy.language import Language\n",
        "\n",
        "# Whitespace tokenizer (copied from Kris' \"Accuracy_LL_2023021.py\")\n",
        "class WhitespaceTokenizer(object):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __call__(self, text):\n",
        "        words = text.split(' ')\n",
        "        # All tokens 'own' a subsequent space character in this tokenizer\n",
        "        spaces = [True] * len(words)\n",
        "        return Doc(self.vocab, words=words, spaces=spaces)\n",
        "\n",
        "# Custom Sentence Boundaries (Force spacy to use our sentence tokenization)\n",
        "@Language.component(\"single_sent\")\n",
        "def custom_sent(doc):\n",
        "    for token in doc:\n",
        "        if token.i == 0:\n",
        "            doc[token.i].is_sent_start = True\n",
        "        else:\n",
        "            doc[token.i].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "# Load the model and modify the pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
        "nlp.add_pipe(\"single_sent\", before='parser') # ensure sentence tokenization is the same as input text\n",
        "print(nlp.pipe_names)  # Should show 'single_sent' before 'parser'\n",
        "\n",
        "def extract_all_sentences(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        data = file.read()\n",
        "    return data.strip().split(\"\\n\\n\")\n",
        "\n",
        "def parse_conllu(sentence):\n",
        "    lines = sentence.split(\"\\n\")\n",
        "    tokens, pos_tags = [], []\n",
        "    for line in lines:\n",
        "        if not line.startswith(\"#\"):\n",
        "            parts = line.split(\"\\t\")\n",
        "            tokens.append(parts[1])\n",
        "            pos_tags.append(parts[3])\n",
        "    return tokens, pos_tags\n",
        "\n",
        "def process_sentences_with_lemmas(all_sentences):\n",
        "    output_data = []\n",
        "    for sentence_chunk in all_sentences:\n",
        "        tokens, custom_pos_tags = parse_conllu(sentence_chunk)\n",
        "        doc = nlp(\" \".join(tokens))\n",
        "\n",
        "        # Update POS tags for all tokens\n",
        "        for token, pos in zip(doc, custom_pos_tags):\n",
        "            token.pos_ = pos\n",
        "\n",
        "        # Only modify lemmas for specific sentences\n",
        "        if '# dataset = en_eslspok' in sentence_chunk or '# dataset = en_eslwrit' in sentence_chunk:\n",
        "            lines = sentence_chunk.split(\"\\n\")\n",
        "            for idx, token in enumerate(doc):\n",
        "                parts = lines[idx + len(lines) - len(doc)].split(\"\\t\")\n",
        "                parts[2] = token.lemma_\n",
        "                lines[idx + len(lines) - len(doc)] = \"\\t\".join(parts)\n",
        "            updated_chunk = \"\\n\".join(lines)\n",
        "            output_data.append(updated_chunk)\n",
        "        else:\n",
        "            output_data.append(sentence_chunk)  # No changes, add the original sentence chunk\n",
        "\n",
        "    return \"\\n\\n\".join(output_data)\n",
        "\n",
        "filename = \"/content/cleaned_silverSentences_database20231128.txt\"\n",
        "extracted_sentences = extract_all_sentences(filename)\n",
        "processed_data = process_sentences_with_lemmas(extracted_sentences)\n",
        "\n",
        "output_file = \"output_lemma_silver.conllu\"\n",
        "with open(output_file, 'w') as file:\n",
        "    file.write(processed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csuk8SExBZud",
        "outputId": "828ac263-2295-4d91-922c-877712cd3ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'tagger', 'single_sent', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentences_by_section(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        data = file.read()\n",
        "\n",
        "    all_sentences = data.strip().split(\"\\n\\n\")\n",
        "\n",
        "    test_sentences = []\n",
        "    train_sentences = []\n",
        "    dev_sentences = []\n",
        "\n",
        "    for sentence_chunk in all_sentences:\n",
        "        if '# section = test' in sentence_chunk:\n",
        "            test_sentences.append(sentence_chunk)\n",
        "        elif '# section = train' in sentence_chunk:\n",
        "            train_sentences.append(sentence_chunk)\n",
        "        elif '# section = dev' in sentence_chunk:\n",
        "            dev_sentences.append(sentence_chunk)\n",
        "\n",
        "    return test_sentences, train_sentences, dev_sentences\n",
        "\n",
        "def write_sentences_to_file(sentences, output_file):\n",
        "    with open(output_file, 'w') as file:\n",
        "        file.write(\"\\n\\n\".join(sentences))\n",
        "\n",
        "filename = \"output_lemma.conllu\"\n",
        "\n",
        "test_sentences, train_sentences, dev_sentences = extract_sentences_by_section(filename)\n",
        "\n",
        "write_sentences_to_file(test_sentences, \"test.conllu\")\n",
        "write_sentences_to_file(train_sentences, \"train.conllu\")\n",
        "write_sentences_to_file(dev_sentences, \"dev.conllu\")"
      ],
      "metadata": {
        "id": "QI6tOVoIC7km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJPDQyov2LBo"
      },
      "outputs": [],
      "source": [
        "def is_sentence_valid(sentence):\n",
        "\n",
        "    # check whether the sentences in the correct form\n",
        "    # HS: in train.conllu (# text = t r u t h o u t | Perspective was problematic)\n",
        "\n",
        "    for token in sentence:\n",
        "        parts = token.split(\"|\")\n",
        "        if len(parts) != 3:\n",
        "            print(f\"Invalid token format: {token}\")\n",
        "            return False\n",
        "        word, pos, tag = parts\n",
        "        if not word or not pos or not tag:\n",
        "            print(f\"Empty field detected in token: {token}\")\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def convert_to_iob_format(input_filename, output_filename):\n",
        "    with open(input_filename, 'r') as infile, open(output_filename, 'w') as outfile:\n",
        "        sentence = []\n",
        "\n",
        "        for line in infile:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line.startswith(\"#\"):\n",
        "                if line.startswith(\"# text\"):\n",
        "                    if sentence:\n",
        "                        if is_sentence_valid(sentence):\n",
        "                            outfile.write(' '.join(sentence) + \"\\n\")\n",
        "                        else:\n",
        "                            print(f\"Invalid sentence detected: {' '.join(sentence)}\")\n",
        "                        sentence = []\n",
        "                continue\n",
        "\n",
        "            # blank line\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            columns = line.split(\"\\t\")\n",
        "            word = columns[1]\n",
        "            pos = columns[4]\n",
        "            tag = columns[9]\n",
        "\n",
        "            if tag == \"_\":\n",
        "                tag = \"O\"\n",
        "            else:\n",
        "                tag = f\"I-{tag}\"\n",
        "\n",
        "            # Append the word|POS|TAG to the current sentence list\n",
        "            sentence.append(f\"{word}|{pos}|{tag}\")\n",
        "\n",
        "        if sentence:\n",
        "            if is_sentence_valid(sentence):\n",
        "                outfile.write(' '.join(sentence) + \"\\n\")\n",
        "            else:\n",
        "                print(f\"Invalid sentence detected at the end: {' '.join(sentence)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"dev.conllu\"\n",
        "    output_file = \"dev.iob\"\n",
        "    convert_to_iob_format(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy convert assets corpus --n-sents 1"
      ],
      "metadata": {
        "id": "G-OEAGF35D_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/config.cfg --output /content/output --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssRyJegG5KPN",
        "outputId": "42ff7f7c-2b6e-4472-92eb-062986bf88cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-06 22:22:01.664597: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-06 22:22:01.664655: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-06 22:22:01.664690: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-06 22:22:02.798085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Saving to output directory: /content/output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        9365.19    528.29    0.00    0.00    0.00    0.00\n",
            "  0     200      250223.89  45193.92   82.36   82.16   82.56    0.82\n",
            "  1     400        3803.03   4322.51   89.64   89.13   90.16    0.90\n",
            "  1     600        2077.93   3204.77   90.79   90.52   91.07    0.91\n",
            "  2     800        1737.94   2690.35   90.34   89.22   91.49    0.90\n",
            "  2    1000        1402.23   2195.23   91.11   91.02   91.19    0.91\n",
            "  3    1200        1235.35   1930.63   90.23   89.83   90.64    0.90\n",
            "  3    1400        1031.54   1603.81   90.39   89.95   90.83    0.90\n",
            "  4    1600         866.67   1347.89   90.27   89.83   90.70    0.90\n",
            "  4    1800         781.72   1200.02   90.70   90.16   91.25    0.91\n",
            "  5    2000         663.80   1016.37   90.97   90.75   91.19    0.91\n",
            "  5    2200         622.53    933.81   89.66   89.52   89.79    0.90\n",
            "  6    2400         450.16    677.25   90.73   89.92   91.56    0.91\n",
            "  6    2600         473.33    706.22   89.97   89.48   90.46    0.90\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "/content/output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy evaluate output/model-best corpus/test.spacy --output output/test_metrics.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8pXF0no6tke",
        "outputId": "12df6dc5-0755-431e-fdac-a9394592a63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-06 22:45:23.460555: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-06 22:45:23.460620: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-06 22:45:23.460659: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-06 22:45:24.700295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   90.33\n",
            "NER R   90.81\n",
            "NER F   90.57\n",
            "SPEED   168  \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                 P       R       F\n",
            "ATTR         97.28   94.84   96.04\n",
            "TRAN_S       92.09   94.20   93.14\n",
            "DITRAN       89.55   85.71   87.59\n",
            "INTRAN_MOT   82.12   83.22   82.67\n",
            "INTRAN_S     81.18   80.47   80.82\n",
            "PASSIVE      88.89   91.89   90.37\n",
            "INTRAN_RES   83.33   55.56   66.67\n",
            "TRAN_RES     76.15   94.32   84.26\n",
            "CAUS_MOT     84.62   76.24   80.21\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to output/test_metrics.json\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsFl7CkkAkFr",
        "outputId": "bbd709c6-b580-40c5-f6f1-ccf42ba3d6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oazE7h7BJpx",
        "outputId": "a4ff04bf-6138-41da-9360-8cca49f5b91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model-best  model-last\ttest_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/My Drive/asc-train-231206\"\n",
        "!cp -r \"/content/output\" \"/content/drive/My Drive/asc-train-231206/\" #save"
      ],
      "metadata": {
        "id": "4jscZq20BOfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "id": "duawiDxhU6YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy_transformers"
      ],
      "metadata": {
        "id": "wFXtXbUb2c2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "model_path = '/content/drive/MyDrive/asc-train-231206/output/model-best'\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "nlp = spacy.load(model_path)\n",
        "\n",
        "# Test the model\n",
        "doc = nlp(\"This is a test sentence.\")\n",
        "print([token.text for token in doc])"
      ],
      "metadata": {
        "id": "TId0MuZYM2XI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50866b7c-bf38-480f-92a7-aa3fca6b5363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'test', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gold3"
      ],
      "metadata": {
        "id": "kDbpiFOcWOAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_ner(model, text):\n",
        "    doc = model(text)\n",
        "    for ent in doc.ents:\n",
        "        print(f\"{ent.text} ({ent.label_})\")\n",
        "\n",
        "# Test the model\n",
        "sentences = [\n",
        "    \"Anita threw the hammer.\",\n",
        "    \"Michelle got the book.\",\n",
        "    \"Barbara sliced the bread.\",\n",
        "    \"Audrey took the watch.\",\n",
        "    \"Chris threw Linda the pencil.\",\n",
        "    \"Beth got Liz an invitation.\",\n",
        "    \"Jennifer sliced Terry an apple.\",\n",
        "    \"Paula took Sue a message.\",\n",
        "    \"Pat threw the keys on the roof.\",\n",
        "    \"Laura got the ball into the net.\",\n",
        "    \"Meg sliced the ham onto the plate.\",\n",
        "    \"Kim took the rose into the house.\",\n",
        "    \"Dana got the mattress inflated.\",\n",
        "    \"Nancy sliced the tire open.\",\n",
        "    \"Lyn threw the box apart.\",\n",
        "    \"Rachel took the wall down.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(f\"Testing: {sentence}\")\n",
        "    test_ner(nlp, sentence)\n",
        "    print(\"-----------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2l1qKsMOFF8",
        "outputId": "bb88e2bb-f8af-48bc-d6e7-4c688ae82ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: Anita threw the hammer.\n",
            "threw (TRAN_S)\n",
            "-----------------------------\n",
            "Testing: Michelle got the book.\n",
            "got (TRAN_S)\n",
            "-----------------------------\n",
            "Testing: Barbara sliced the bread.\n",
            "sliced (TRAN_S)\n",
            "-----------------------------\n",
            "Testing: Audrey took the watch.\n",
            "took (TRAN_S)\n",
            "-----------------------------\n",
            "Testing: Chris threw Linda the pencil.\n",
            "threw (DITRAN)\n",
            "-----------------------------\n",
            "Testing: Beth got Liz an invitation.\n",
            "got (DITRAN)\n",
            "-----------------------------\n",
            "Testing: Jennifer sliced Terry an apple.\n",
            "sliced (DITRAN)\n",
            "-----------------------------\n",
            "Testing: Paula took Sue a message.\n",
            "took (TRAN_S)\n",
            "-----------------------------\n",
            "Testing: Pat threw the keys on the roof.\n",
            "threw (CAUS_MOT)\n",
            "-----------------------------\n",
            "Testing: Laura got the ball into the net.\n",
            "got (CAUS_MOT)\n",
            "-----------------------------\n",
            "Testing: Meg sliced the ham onto the plate.\n",
            "sliced (CAUS_MOT)\n",
            "-----------------------------\n",
            "Testing: Kim took the rose into the house.\n",
            "took (CAUS_MOT)\n",
            "-----------------------------\n",
            "Testing: Dana got the mattress inflated.\n",
            "got (TRAN_RES)\n",
            "inflated (PASSIVE)\n",
            "-----------------------------\n",
            "Testing: Nancy sliced the tire open.\n",
            "sliced (TRAN_RES)\n",
            "-----------------------------\n",
            "Testing: Lyn threw the box apart.\n",
            "threw (TRAN_RES)\n",
            "-----------------------------\n",
            "Testing: Rachel took the wall down.\n",
            "took (CAUS_MOT)\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gold2"
      ],
      "metadata": {
        "id": "-WXqUHjbtEjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "model_path = '/content/drive/MyDrive/asc-train-231119/output/model-best'\n",
        "\n",
        "# Load the model\n",
        "nlp = spacy.load(\"en_core_web_trf\")\n",
        "nlp = spacy.load(model_path)\n",
        "\n",
        "# Test the model\n",
        "sentences = [\n",
        "    \"Anita threw the hammer.\",\n",
        "    \"Michelle got the book.\",\n",
        "    \"Barbara sliced the bread.\",\n",
        "    \"Audrey took the watch.\",\n",
        "    \"Chris threw Linda the pencil.\",\n",
        "    \"Beth got Liz an invitation.\",\n",
        "    \"Jennifer sliced Terry an apple.\",\n",
        "    \"Paula took Sue a message.\",\n",
        "    \"Pat threw the keys on the roof.\",\n",
        "    \"Laura got the ball into the net.\",\n",
        "    \"Meg sliced the ham onto the plate.\",\n",
        "    \"Kim took the rose into the house.\",\n",
        "    \"Dana got the mattress inflated.\",\n",
        "    \"Nancy sliced the tire open.\",\n",
        "    \"Lyn threw the box apart.\",\n",
        "    \"Rachel took the wall down.\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(f\"Testing: {sentence}\")\n",
        "    test_ner(nlp, sentence)\n",
        "    print(\"-----------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy-ScPngs3wI",
        "outputId": "a2f14099-0193-4d19-efbd-5cf5486eb394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: Anita threw the hammer.\n",
            "threw (TRAN_S)\n",
            "-----------------------------\n",
            "Testing: Michelle got the book.\n",
            "got (TRAN_S)\n",
            "-----------------------------\n",
            "Testing: Barbara sliced the bread.\n",
            "sliced (TRAN_S)\n",
            "-----------------------------\n",
            "Testing: Audrey took the watch.\n",
            "took (TRAN_S)\n",
            "-----------------------------\n",
            "Testing: Chris threw Linda the pencil.\n",
            "threw (DITRAN)\n",
            "-----------------------------\n",
            "Testing: Beth got Liz an invitation.\n",
            "got (DITRAN)\n",
            "-----------------------------\n",
            "Testing: Jennifer sliced Terry an apple.\n",
            "sliced (DITRAN)\n",
            "-----------------------------\n",
            "Testing: Paula took Sue a message.\n",
            "took (DITRAN)\n",
            "-----------------------------\n",
            "Testing: Pat threw the keys on the roof.\n",
            "threw (CAUS_MOT)\n",
            "-----------------------------\n",
            "Testing: Laura got the ball into the net.\n",
            "got (CAUS_MOT)\n",
            "-----------------------------\n",
            "Testing: Meg sliced the ham onto the plate.\n",
            "sliced (CAUS_MOT)\n",
            "-----------------------------\n",
            "Testing: Kim took the rose into the house.\n",
            "took (CAUS_MOT)\n",
            "-----------------------------\n",
            "Testing: Dana got the mattress inflated.\n",
            "got (TRAN_RES)\n",
            "inflated (PASSIVE)\n",
            "-----------------------------\n",
            "Testing: Nancy sliced the tire open.\n",
            "sliced (TRAN_RES)\n",
            "-----------------------------\n",
            "Testing: Lyn threw the box apart.\n",
            "threw (TRAN_RES)\n",
            "-----------------------------\n",
            "Testing: Rachel took the wall down.\n",
            "took (TRAN_RES)\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading and processing the three JSON files\n",
        "\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/asc-train-231031/output1/gold1.json',\n",
        "    '/content/drive/MyDrive/asc-train-231031/output2/gold+silver.json',\n",
        "    '/content/drive/MyDrive/asc-train-231119/output/gold2.json'\n",
        "]\n",
        "\n",
        "# Function to process each file\n",
        "def process_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    metrics = {\n",
        "        \"ents_p\": data[\"ents_p\"],\n",
        "        \"ents_r\": data[\"ents_r\"],\n",
        "        \"ents_f\": data[\"ents_f\"]\n",
        "    }\n",
        "\n",
        "    for key, value in data[\"ents_per_type\"].items():\n",
        "        metrics[f'{key}_p'] = value['p']\n",
        "        metrics[f'{key}_r'] = value['r']\n",
        "        metrics[f'{key}_f'] = value['f']\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Processing each file\n",
        "metrics_data = [process_file(path) for path in file_paths]\n",
        "\n",
        "# Creating a DataFrame from the processed data\n",
        "df = pd.DataFrame(metrics_data)\n",
        "df.columns = [col if \"_\" not in col else col.split(\"_\")[0] + \" \" + col.split(\"_\")[1] for col in df.columns]\n",
        "df.index = ['Gold1', 'Gold+Silver', 'Gold2']\n",
        "\n",
        "# Formatting the values to two decimal places\n",
        "df = df.applymap(lambda x: f'{x:.2f}' if isinstance(x, float) else x)\n",
        "\n",
        "# Transposing the DataFrame\n",
        "transposed_df = df.transpose()\n",
        "\n",
        "# Path for the CSV file\n",
        "csv_file_path = '/content/comparison_metrics.csv'\n",
        "\n",
        "# Saving the transposed DataFrame to a CSV file\n",
        "transposed_df.to_csv(csv_file_path, index=True)"
      ],
      "metadata": {
        "id": "3mVJukuAZNJX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}